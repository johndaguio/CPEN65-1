{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN4pAtG5Kiv/YRm60bWo9ih",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/johndaguio/CPEN65-1/blob/main/Lab_Exercise_2_Classification%20FINAL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "param_grid = [{'weights': [\"uniform\", \"distance\"], 'n_neighbors': [3, 4, 5]}]\n",
        "\n",
        "knn_clf = KNeighborsClassifier()\n",
        "grid_search = GridSearchCV(knn_clf, param_grid, cv=5, verbose=3)\n",
        "from sklearn.datasets import fetch_openml\n",
        "mnist = fetch_openml('mnist_784', version=1, as_frame=False, parser=\"auto\")\n",
        "mnist.keys()\n",
        "X, y = mnist[\"data\"], mnist[\"target\"]\n",
        "X.shape\n",
        "y.shape\n",
        "28 * 28\n",
        "X_train, X_test, y_train, y_test = (X[:60000], X[60000:], y[:60000], y[60000:])\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "grid_search.best_params_\n",
        "\n",
        "grid_search.best_score_\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "y_pred = grid_search.predict(X_test)\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vs8jEfCkLym0",
        "outputId": "f932b262-1a8c-4ee3-dcf6-4ae77f065e34"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
            "[CV 1/5] END ....n_neighbors=3, weights=uniform;, score=0.972 total time=  35.6s\n",
            "[CV 2/5] END ....n_neighbors=3, weights=uniform;, score=0.971 total time=  33.9s\n",
            "[CV 3/5] END ....n_neighbors=3, weights=uniform;, score=0.969 total time=  33.7s\n",
            "[CV 4/5] END ....n_neighbors=3, weights=uniform;, score=0.969 total time=  33.1s\n",
            "[CV 5/5] END ....n_neighbors=3, weights=uniform;, score=0.970 total time=  32.2s\n",
            "[CV 1/5] END ...n_neighbors=3, weights=distance;, score=0.972 total time=  33.2s\n",
            "[CV 2/5] END ...n_neighbors=3, weights=distance;, score=0.972 total time=  33.3s\n",
            "[CV 3/5] END ...n_neighbors=3, weights=distance;, score=0.970 total time=  33.1s\n",
            "[CV 4/5] END ...n_neighbors=3, weights=distance;, score=0.970 total time=  32.3s\n",
            "[CV 5/5] END ...n_neighbors=3, weights=distance;, score=0.971 total time=  32.9s\n",
            "[CV 1/5] END ....n_neighbors=4, weights=uniform;, score=0.969 total time=  33.1s\n",
            "[CV 2/5] END ....n_neighbors=4, weights=uniform;, score=0.968 total time=  33.9s\n",
            "[CV 3/5] END ....n_neighbors=4, weights=uniform;, score=0.968 total time=  32.8s\n",
            "[CV 4/5] END ....n_neighbors=4, weights=uniform;, score=0.967 total time=  34.2s\n",
            "[CV 5/5] END ....n_neighbors=4, weights=uniform;, score=0.970 total time=  34.2s\n",
            "[CV 1/5] END ...n_neighbors=4, weights=distance;, score=0.973 total time=  33.5s\n",
            "[CV 2/5] END ...n_neighbors=4, weights=distance;, score=0.972 total time=  34.3s\n",
            "[CV 3/5] END ...n_neighbors=4, weights=distance;, score=0.970 total time=  32.5s\n",
            "[CV 4/5] END ...n_neighbors=4, weights=distance;, score=0.971 total time=  34.1s\n",
            "[CV 5/5] END ...n_neighbors=4, weights=distance;, score=0.972 total time=  33.7s\n",
            "[CV 1/5] END ....n_neighbors=5, weights=uniform;, score=0.970 total time=  33.2s\n",
            "[CV 2/5] END ....n_neighbors=5, weights=uniform;, score=0.970 total time=  34.1s\n",
            "[CV 3/5] END ....n_neighbors=5, weights=uniform;, score=0.969 total time=  32.6s\n",
            "[CV 4/5] END ....n_neighbors=5, weights=uniform;, score=0.968 total time=  34.4s\n",
            "[CV 5/5] END ....n_neighbors=5, weights=uniform;, score=0.969 total time=  34.1s\n",
            "[CV 1/5] END ...n_neighbors=5, weights=distance;, score=0.970 total time=  33.3s\n",
            "[CV 2/5] END ...n_neighbors=5, weights=distance;, score=0.971 total time=  33.9s\n",
            "[CV 3/5] END ...n_neighbors=5, weights=distance;, score=0.970 total time=  32.4s\n",
            "[CV 4/5] END ...n_neighbors=5, weights=distance;, score=0.969 total time=  33.9s\n",
            "[CV 5/5] END ...n_neighbors=5, weights=distance;, score=0.971 total time=  34.1s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9714"
            ]
          },
          "metadata": {},
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.ndimage import shift\n",
        "def shift_image(image, dx, dy):\n",
        "    image = image.reshape((28, 28))\n",
        "    shifted_image = shift(image, [dy, dx], cval=0, mode=\"constant\")\n",
        "    return shifted_image.reshape([-1])\n",
        "    image = X_train[1000]\n",
        "    shifted_image_down = shift_image(image, 0, 5)\n",
        "    shifted_image_left = shift_image(image, -5, 0)\n",
        "\n",
        "    plt.figure(figsize=(12,3))\n",
        "    plt.subplot(131)\n",
        "    plt.title(\"Original\", fontsize=14)\n",
        "    plt.imshow(image.reshape(28, 28), interpolation=\"nearest\", cmap=\"Greys\")\n",
        "    plt.subplot(132)\n",
        "    plt.title(\"Shifted down\", fontsize=14)\n",
        "    plt.imshow(shifted_image_down.reshape(28, 28), interpolation=\"nearest\", cmap=\"Greys\")\n",
        "    plt.subplot(133)\n",
        "    plt.title(\"Shifted left\", fontsize=14)\n",
        "    plt.imshow(shifted_image_left.reshape(28, 28), interpolation=\"nearest\", cmap=\"Greys\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "epGRFU_QlHl6"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import urllib.request\n",
        "\n",
        "TITANIC_PATH = os.path.join(\"datasets\", \"titanic\")\n",
        "DOWNLOAD_URL = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/datasets/titanic/\"\n",
        "\n",
        "def fetch_titanic_data(url=DOWNLOAD_URL, path=TITANIC_PATH):\n",
        "    if not os.path.isdir(path):\n",
        "        os.makedirs(path)\n",
        "    for filename in (\"train.csv\", \"test.csv\"):\n",
        "        filepath = os.path.join(path, filename)\n",
        "        if not os.path.isfile(filepath):\n",
        "            print(\"Downloading\", filename)\n",
        "            urllib.request.urlretrieve(url + filename, filepath)\n",
        "\n",
        "fetch_titanic_data()    \n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def load_titanic_data(filename, titanic_path=TITANIC_PATH):\n",
        "    csv_path = os.path.join(titanic_path, filename)\n",
        "    return pd.read_csv(csv_path)\n",
        "\n",
        "    train_data = load_titanic_data(\"train.csv\")\n",
        "    test_data = load_titanic_data(\"test.csv\")\n",
        "    train_data.head()\n",
        "    train_data = train_data.set_index(\"PassengerId\")\n",
        "    test_data = test_data.set_index(\"PassengerId\")\n",
        "    train_data.info()\n",
        "    train_data[train_data[\"Sex\"]==\"female\"][\"Age\"].median()\n",
        "    train_data.describe()\n",
        "    train_data[\"Survived\"].value_counts()\n",
        "    train_data[\"Pclass\"].value_counts()\n",
        "    train_data[\"Sex\"].value_counts()\n",
        "    train_data[\"Embarked\"].value_counts()\n",
        "    from sklearn.pipeline import Pipeline\n",
        "    from sklearn.impute import SimpleImputer\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "    num_pipeline = Pipeline([\n",
        "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "        (\"scaler\", StandardScaler())])\n",
        "    from sklearn.preprocessing import OneHotEncoder\n",
        "    from sklearn.compose import ColumnTransformer\n",
        "\n",
        "    num_attribs = [\"Age\", \"SibSp\", \"Parch\", \"Fare\"]\n",
        "    cat_attribs = [\"Pclass\", \"Sex\", \"Embarked\"]\n",
        "\n",
        "    preprocess_pipeline = ColumnTransformer([(\"num\", num_pipeline, num_attribs),(\"cat\", cat_pipeline, cat_attribs),])\n",
        "    X_train = preprocess_pipeline.fit_transform(train_data[num_attribs + cat_attribs])\n",
        "    y_train = train_data[\"Survived\"]\n",
        "    from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "    forest_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    forest_clf.fit(X_train, y_train)\n",
        "\n",
        "    X_test = preprocess_pipeline.transform(test_data[num_attribs + cat_attribs])\n",
        "    y_pred = forest_clf.predict(X_test)\n",
        "\n",
        "    from sklearn.model_selection import cross_val_score\n",
        "\n",
        "    forest_scores = cross_val_score(forest_clf, X_train, y_train, cv=10)\n",
        "    forest_scores.mean()\n",
        "\n",
        "    from sklearn.svm import SVC\n",
        "\n",
        "    svm_clf = SVC(gamma=\"auto\")\n",
        "    svm_scores = cross_val_score(svm_clf, X_train, y_train, cv=10)\n",
        "    svm_scores.mean()\n",
        "\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.plot([1]*10, svm_scores, \".\")\n",
        "    plt.plot([2]*10, forest_scores, \".\")\n",
        "    plt.boxplot([svm_scores, forest_scores], labels=(\"SVM\",\"Random Forest\"))\n",
        "    plt.ylabel(\"Accuracy\", fontsize=14)\n",
        "    plt.show()\n",
        "\n",
        "    train_data[\"AgeBucket\"] = train_data[\"Age\"] // 15 * 15\n",
        "    train_data[[\"AgeBucket\", \"Survived\"]].groupby(['AgeBucket']).mean()\n",
        "\n",
        "    train_data[\"RelativesOnboard\"] = train_data[\"SibSp\"] + train_data[\"Parch\"]\n",
        "    train_data[[\"RelativesOnboard\", \"Survived\"]].groupby(['RelativesOnboard']).mean()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_ErpwbWQ3Ic",
        "outputId": "7298a21b-a2c2-493d-992b-2a938da9b57b"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading train.csv\n",
            "Downloading test.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tarfile\n",
        "import urllib.request\n",
        "\n",
        "DOWNLOAD_ROOT = \"http://spamassassin.apache.org/old/publiccorpus/\"\n",
        "HAM_URL = DOWNLOAD_ROOT + \"20030228_easy_ham.tar.bz2\"\n",
        "SPAM_URL = DOWNLOAD_ROOT + \"20030228_spam.tar.bz2\"\n",
        "SPAM_PATH = os.path.join(\"datasets\", \"spam\")\n",
        "\n",
        "def fetch_spam_data(ham_url=HAM_URL, spam_url=SPAM_URL, spam_path=SPAM_PATH):\n",
        "    if not os.path.isdir(spam_path):\n",
        "        os.makedirs(spam_path)\n",
        "    for filename, url in ((\"ham.tar.bz2\", ham_url), (\"spam.tar.bz2\", spam_url)):\n",
        "        path = os.path.join(spam_path, filename)\n",
        "        if not os.path.isfile(path):\n",
        "            urllib.request.urlretrieve(url, path)\n",
        "        tar_bz2_file = tarfile.open(path)\n",
        "        tar_bz2_file.extractall(path=spam_path)\n",
        "        tar_bz2_file.close()\n",
        "\n",
        "fetch_spam_data()\n",
        "\n",
        "HAM_DIR = os.path.join(SPAM_PATH, \"easy_ham\")\n",
        "SPAM_DIR = os.path.join(SPAM_PATH, \"spam\")\n",
        "ham_filenames = [name for name in sorted(os.listdir(HAM_DIR)) if len(name) > 20]\n",
        "spam_filenames = [name for name in sorted(os.listdir(SPAM_DIR)) if len(name) > 20]\n",
        "len(ham_filenames)\n",
        "len(spam_filenames)\n",
        "\n",
        "import email\n",
        "import email.policy\n",
        "\n",
        "def load_email(is_spam, filename, spam_path=SPAM_PATH):\n",
        "    directory = \"spam\" if is_spam else \"easy_ham\"\n",
        "    with open(os.path.join(spam_path, directory, filename), \"rb\") as f:\n",
        "        return email.parser.BytesParser(policy=email.policy.default).parse(f)\n",
        "        ham_emails = [load_email(is_spam=False, filename=name) for name in ham_filenames]\n",
        "        spam_emails = [load_email(is_spam=True, filename=name) for name in spam_filenames]\n",
        "        print(ham_emails[1].get_content().strip())\n",
        "        print(spam_emails[6].get_content().strip())\n",
        "def get_email_structure(email):\n",
        "    if isinstance(email, str):\n",
        "        return email\n",
        "    payload = email.get_payload()\n",
        "    if isinstance(payload, list):\n",
        "        return \"multipart({})\".format(\", \".join([get_email_structure(sub_email) for sub_email in payload]))\n",
        "    else:\n",
        "        return email.get_content_type()\n",
        "from collections import Counter\n",
        "\n",
        "def structures_counter(emails):\n",
        "    structures = Counter()\n",
        "    for email in emails:\n",
        "        structure = get_email_structure(email)\n",
        "        structures[structure] += 1\n",
        "    return structures\n",
        "    structures_counter(ham_emails).most_common()\n",
        "    structures_counter(spam_emails).most_common()\n",
        "    for header, value in spam_emails[0].items():\n",
        "     print(header,\":\",value)\n",
        "    spam_emails[0][\"Subject\"]\n",
        "    import numpy as np\n",
        "    from sklearn.model_selection import train_test_split\n",
        "\n",
        "    X = np.array(ham_emails + spam_emails, dtype=object)\n",
        "    y = np.array([0] * len(ham_emails) + [1] * len(spam_emails))\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    import re\n",
        "    from html import unescape\n",
        "\n",
        "def html_to_plain_text(html):\n",
        "    text = re.sub('<head.*?>.*?</head>', '', html, flags=re.M | re.S | re.I)\n",
        "    text = re.sub('<a\\s.*?>', ' HYPERLINK ', text, flags=re.M | re.S | re.I)\n",
        "    text = re.sub('<.*?>', '', text, flags=re.M | re.S)\n",
        "    text = re.sub(r'(\\s*\\n)+', '\\n', text, flags=re.M | re.S)\n",
        "    return unescape(text)\n",
        "    html_spam_emails = [email for email in X_train[y_train==1]\n",
        "                    if get_email_structure(email) == \"text/html\"]\n",
        "    sample_html_spam = html_spam_emails[7]\n",
        "    print(sample_html_spam.get_content().strip()[:1000], \"...\")\n",
        "    print(html_to_plain_text(sample_html_spam.get_content())[:1000], \"...\")\n",
        "    \n",
        "def email_to_text(email):\n",
        "    html = None\n",
        "    for part in email.walk():\n",
        "        ctype = part.get_content_type()\n",
        "        if not ctype in (\"text/plain\", \"text/html\"):\n",
        "            continue\n",
        "        try:\n",
        "            content = part.get_content()\n",
        "        except: # in case of encoding issues\n",
        "            content = str(part.get_payload())\n",
        "        if ctype == \"text/plain\":\n",
        "            return content\n",
        "        else:\n",
        "            html = content\n",
        "    if html:\n",
        "        return html_to_plain_text(html)\n",
        "        print(email_to_text(sample_html_spam)[:100], \"...\")\n",
        "        try:\n",
        "            import nltk\n",
        "\n",
        "            stemmer = nltk.PorterStemmer()\n",
        "            for word in (\"Computations\", \"Computation\", \"Computing\", \"Computed\", \"Compute\", \"Compulsive\"):\n",
        "                print(word, \"=>\", stemmer.stem(word))\n",
        "        except ImportError:\n",
        "            print(\"Error: stemming requires the NLTK module.\")\n",
        "            stemmer = None\n",
        "\n",
        "        if IS_COLAB or IS_KAGGLE:\n",
        "         %pip install -q -U urlextract\n",
        "\n",
        "try:\n",
        "        import urlextract # may require an Internet connection to download root domain names\n",
        "    \n",
        "        url_extractor = urlextract.URLExtract()\n",
        "        print(url_extractor.find_urls(\"Will it detect github.com and https://youtu.be/7Pq-S557XQU?t=3m32s\"))\n",
        "        \n",
        "except ImportError:\n",
        "        print(\"Error: replacing URLs requires the urlextract module.\")\n",
        "        url_extractor = None\n",
        "\n",
        "class EmailToWordCounterTransformer():\n",
        "    def __init__(self, strip_headers=True, lower_case=True, remove_punctuation=True,\n",
        "                 replace_urls=True, replace_numbers=True, stemming=True):\n",
        "        self.strip_headers = strip_headers\n",
        "        self.lower_case = lower_case\n",
        "        self.remove_punctuation = remove_punctuation\n",
        "        self.replace_urls = replace_urls\n",
        "        self.replace_numbers = replace_numbers\n",
        "        self.stemming = stemming\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "    def transform(self, X, y=None):\n",
        "        X_transformed = []\n",
        "        for email in X:\n",
        "            text = email_to_text(email) or \"\"\n",
        "            if self.lower_case:\n",
        "                text = text.lower()\n",
        "            if self.replace_urls and url_extractor is not None:\n",
        "                urls = list(set(url_extractor.find_urls(text)))\n",
        "                urls.sort(key=lambda url: len(url), reverse=True)\n",
        "                for url in urls:\n",
        "                    text = text.replace(url, \" URL \")\n",
        "            if self.replace_numbers:\n",
        "                text = re.sub(r'\\d+(?:\\.\\d*)?(?:[eE][+-]?\\d+)?', 'NUMBER', text)\n",
        "            if self.remove_punctuation:\n",
        "                text = re.sub(r'\\W+', ' ', text, flags=re.M)\n",
        "            word_counts = Counter(text.split())\n",
        "            if self.stemming and stemmer is not None:\n",
        "                stemmed_word_counts = Counter()\n",
        "                for word, count in word_counts.items():\n",
        "                    stemmed_word = stemmer.stem(word)\n",
        "                    stemmed_word_counts[stemmed_word] += count\n",
        "                word_counts = stemmed_word_counts\n",
        "            X_transformed.append(word_counts)\n",
        "        return np.array(X_transformed)\n",
        "        X_few = X_train[:3]\n",
        "        X_few_wordcounts = EmailToWordCounterTransformer().fit_transform(X_few)\n",
        "        X_few_wordcounts\n",
        "\n",
        "    from scipy.sparse import csr_matrix\n",
        "\n",
        "class WordCounterToVectorTransformer():\n",
        "    def __init__(self, vocabulary_size=1000):\n",
        "        self.vocabulary_size = vocabulary_size\n",
        "    def fit(self, X, y=None):\n",
        "        total_count = Counter()\n",
        "        for word_count in X:\n",
        "            for word, count in word_count.items():\n",
        "                total_count[word] += min(count, 10)\n",
        "        most_common = total_count.most_common()[:self.vocabulary_size]\n",
        "        self.vocabulary_ = {word: index + 1 for index, (word, count) in enumerate(most_common)}\n",
        "        return self\n",
        "    def transform(self, X, y=None):\n",
        "        rows = []\n",
        "        cols = []\n",
        "        data = []\n",
        "        for row, word_count in enumerate(X):\n",
        "            for word, count in word_count.items():\n",
        "                rows.append(row)\n",
        "                cols.append(self.vocabulary_.get(word, 0))\n",
        "                data.append(count)\n",
        "        return csr_matrix((data, (rows, cols)), shape=(len(X), self.vocabulary_size + 1))\n",
        "\n",
        "        vocab_transformer = WordCounterToVectorTransformer(vocabulary_size=10)\n",
        "        X_few_vectors = vocab_transformer.fit_transform(X_few_wordcounts)\n",
        "        X_few_vectors\n",
        "        X_few_vectors.toarray()\n",
        "        vocab_transformer.vocabulary_\n",
        "        from sklearn.pipeline import Pipeline\n",
        "\n",
        "        preprocess_pipeline = Pipeline([\n",
        "           (\"email_to_wordcount\", EmailToWordCounterTransformer()),\n",
        "           (\"wordcount_to_vector\", WordCounterToVectorTransformer()),])\n",
        "\n",
        "        X_train_transformed = preprocess_pipeline.fit_transform(X_train)\n",
        "        from sklearn.linear_model import LogisticRegression\n",
        "        from sklearn.model_selection import cross_val_score\n",
        "\n",
        "        log_clf = LogisticRegression(solver=\"lbfgs\", max_iter=1000, random_state=42)\n",
        "        score = cross_val_score(log_clf, X_train_transformed, y_train, cv=3, verbose=3)\n",
        "        score.mean()\n",
        "        from sklearn.metrics import precision_score, recall_score\n",
        "\n",
        "        X_test_transformed = preprocess_pipeline.transform(X_test)\n",
        "\n",
        "        log_clf = LogisticRegression(solver=\"lbfgs\", max_iter=1000, random_state=42)\n",
        "        log_clf.fit(X_train_transformed, y_train)\n",
        "\n",
        "        y_pred = log_clf.predict(X_test_transformed)\n",
        "\n",
        "        print(\"Precision: {:.2f}%\".format(100 * precision_score(y_test, y_pred)))\n",
        "        print(\"Recall: {:.2f}%\".format(100 * recall_score(y_test, y_pred)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NABqcV5KQ5rS",
        "outputId": "4bd25dcb-2081-4095-e541-2666f062eb8b"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: replacing URLs requires the urlextract module.\n"
          ]
        }
      ]
    }
  ]
}